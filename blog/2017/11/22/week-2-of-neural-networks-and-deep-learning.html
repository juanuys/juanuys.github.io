<!DOCTYPE html>
<html lang="en">

  <head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-C41XWQXNNL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-C41XWQXNNL');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Permissions-Policy" content="interest-cohort=()"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  


  <link rel="me" href="https://mastodon.art/@uys">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="theme-color" content="#f6ffd3">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate" type="application/rss+xml" title="Juan Uys"
    href="/feed.xml">

  
  <meta name="robots" content="index, follow" />


  <!-- Primary Meta Tags -->
  <title>Juan Uys</title>
  <meta name="title" content="Juan Uys">
  <meta name="description" content="Witness Juan Uys concerning himself with art, computer programming, and other frivolous endeavours.">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://juanuys.com/">
  <meta property="og:title" content="Juan Uys">
  <meta property="og:description" content="Witness Juan Uys concerning himself with art, computer programming, and other frivolous endeavours.">
  <meta property="og:image" content="https://juanuys.com/assets/about/juanuys.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://juanuys.com/">
  <meta property="twitter:title" content="Juan Uys">
  <meta property="twitter:description" content="Witness Juan Uys concerning himself with art, computer programming, and other frivolous endeavours.">
  <meta property="twitter:image" content="https://juanuys.com/assets/about/juanuys.png">

  <!-- indieweb -->
  <a href="https://juanuys.com/" class="h-card" rel="me">Juan Uys</a>
</head>


  <body>

    <div class="all">
      <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    
    

    
    
    

    
    <a class="site-title" rel="author" href="/blog">
      
      <img src="/assets/index/pixelated.png" alt="Juan Uys" height="40"> /blog
      
    </a>
          

  </div>
</header>


      <main class="page-content" aria-label="Content">
        <div class="wrapper">
          <div class="content">
  <div class="post single">
      <!--
    <div class="meta">
        <p class="item tags small">
            <a href="/tag/nn" class="tag">nn</a><a href="/tag/coursera" class="tag">coursera</a><a href="/tag/deeplearning" class="tag">deeplearning</a>
        </p>
    </div>
    -->

    <h1 class="title">Week 2 of Neural Networks and Deep Learning</h1>

    <div>      
      

      
        <div style="float: left; padding-right: 20px;">tags:</div>

        <div class="tags">
          <ul>
          
          
            <li><a title="tag: nn" href="/tags#nn">nn</a></li>
          
            
          
          
            <li><a title="tag: coursera" href="/tags#coursera">coursera</a></li>
          
            
          
          
            <li><a title="tag: deeplearning" href="/tags#deeplearning">deeplearning</a></li>
          
            
          
          </ul>
        </div>
      
    </div>
    

    <info datetime="2017-11-22">
        2017-11-22
    </info>

    <div class="breaker"></div>
    <div class="body"><h1 id="neural-network-basics">Neural Network Basics</h1>

<p>These are my notes on week #2 of the <a href="https://www.coursera.org/learn/neural-networks-deep-learning/">Introduction to Deep Learning</a> Coursera MOOC.</p>

<h2 id="binary-classification">Binary Classification</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Z8j0R/binary-classification">Video #1</a> basically sets up some notation for showing how a matrix input maps to one bit of information (1 or 0, true or false, i.e. whether a picture is of a hotdog <a href="http://www.foodandwine.com/news/silicon-valleys-hot-dog-identifying-app-very-real">or not</a>).</p>

<h2 id="logistic-regression">Logistic regression</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/LoKih/logistic-regression">Video #2</a> shows how Logistic Regression trumps Linear Regression in the sense that the latter will output values in the range negative to very large (&gt;1), which is not what we want for binary classification. Instead, Logistic regression “clamps” the output to <strong>0 &lt;= y^ &lt;= 1</strong> using a sigmoid function. “y hat”, i.e. <em>y^</em> will be the prediction of <em>y</em>, the “ground truth”.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/logisticregression.png" alt="screenshot" /></p>

<h2 id="logistic-regression-cost-function">Logistic regression cost function</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function">Video #3</a> explains the cost function needed to train parameters <em>w</em> and <em>b</em>.
Square error doesn’t work well as a loss (or error) function, because no global minimum can be obtained. It makes gradient descent not work well at all (your “ball” gets trapped in the first “hole” as it rolls down the hill). We want a function which ends us up with one convex basin, not a squiggly line (many basins).</p>

<p>The cost function measures how well you’re doing over the entire training set.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/logisticregression-costfunction.png" alt="screenshot" /></p>

<h2 id="gradient-descent">Gradient descent</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/A0tBd/gradient-descent">Video #4</a> discusses gradient descent as an “optimisation algorithm” shows how logistic regression can be used as a tiny neural net.</p>

<p>Gradient descent is your “ball rolling to the bottom of the basin”, or final a local optimum for <em>w</em>. It shows how taking the derivative (or the slope) of the curve finds the optimum where the slope is 0 (a flat line underneath the curve). For the multi-dimensional case where we’re finding <em>w</em> AND <em>b</em>, it would be the plane under the basin.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/gradient-descent.png" alt="screenshot" /></p>

<p>The latter part of the video makes a brief note of calculus where you can use <em>d</em> for the derivative of a single-variable function, or <em>squiggly-d</em> as the partial derivative of a multi-variable function, or the cost function, in our case: <em>J(w,b)</em></p>

<h2 id="derivatives">Derivatives</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0ULGt/derivatives">Video #5</a> is just a calculus refresher.</p>

<p>Ditto for <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/oEcPT/more-derivative-examples">Video #6</a>.</p>

<p>If you are in need of a calculus refresher, I suggest supplementing these two videos with <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3blue1brown’s channel</a> or <a href="https://www.khanacademy.org/">Khan Academy</a> and also pay attention to the “chain rule”.</p>

<p>During this course, you’re not expected to derive the derivatives - all the ones you need are provided, or otherwise consult the derivative lookup tables in any calculus textbook.</p>

<h2 id="computation-graph">Computation graph</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph">Video #6</a> talks about the computation graph which explains why neural networks are organised in terms of a forward pass (for the output of the network) and a back propagation step (for the gradients or derivatives). It shows an example of working through a computation graph to compute the function J (the cost function).</p>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph">Video #7</a> goes further with more examples, like computing the derivate of <em>J</em> with respect to <em>v</em>, or <em>J</em> with respect to <em>a</em> in this graph:</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/computation-graph-derivaties-example1.png" alt="screenshot" /></p>

<p>If <em>v</em> or <em>a</em> nudges by a little bit (0.001), then <em>J</em> nudges by 3 times as much.</p>

<p>Ng further shows that nudging <em>b</em> by 0.001 nudges <em>J</em> by 6 times as much, and <em>c</em> 9 times as much.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/computation-graph-derivaties-example2.png" alt="screenshot" /></p>

<p>I like how this video uses a decimal (0.001) change to show how <em>J</em>’s decimal changes, and all-in-all building up good “number sense” for derivatives and forward- and back-propagation.</p>

<p>Also, in Python code, the variable representing “the derivative in terms of VAR” would just be written in code as <em>dvar</em>, e.g. <em>da</em>.</p>

<h2 id="logistic-regression-gradient-descent">Logistic regression gradient descent</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/5sdh6/logistic-regression-gradient-descent">Video #8</a> discusses the key equations needed to compute gradient descent for logistic regression. Ng uses the computation graph to illustrate again.</p>

<p>Recap logistic regression:</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/logistic-regression-recap.png" alt="screenshot" /></p>

<p>The example using the computation graph shows how to get the derivatives of the weights and the learning rate, with which to update the weights and learning rate:</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/logistic-regression-gradient-descent.png" alt="screenshot" /></p>

<h2 id="gradient-descent-on-m-examples">Gradient descent on m examples</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/udiAq/gradient-descent-on-m-examples">Video #9</a> takes the previous single example and applies it to a training set of <em>m</em> examples. (reminder that the cost function <em>J</em> is the loss function <em>L</em> over <em>m</em> examples.)</p>

<p>Ng scribbles down some pseudo-code (for two features: <em>dw1</em> and <em>dw2</em>) and makes a note about using for loops with large data sets, and how the expensiveness of it can be circumvented by vectorisation, which is discussed in the next video.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/pseudocode-gradient-descent-m-examples.png" alt="screenshot" /></p>

<h2 id="vectorisation">Vectorisation</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/NYnog/vectorization">Video #10</a> starts with “vectorisation is the art of getting rid of for loops in code”.</p>

<p>A quick Jupyter example shows the speed-up:</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/vectorisation-jupyter.png" alt="screenshot" /></p>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/ZPlX9/more-vectorization-examples">Video #11</a> elaborates with more Python examples. Numpy will have element-wise functions for each of <em>exp</em>, <em>log</em>, <em>abs</em>, etc so you’ll never need to loop.</p>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/moUlO/vectorizing-logistic-regression">Video #12</a> talks about vectorising logistic regression.</p>

<p>Just a note: Activation = forward propagation = predictions, terms Ng uses interchangeably. <em>a</em> is also denoted <em>y^</em> (y hat being the prediction) sometimes.</p>

<p>“1 by m” matrix is just a “row vector”, or “an m-dimensional row vector”.</p>

<p>Here he shows how you can get <em>z1</em>, <em>z2</em>, …, <em>zm</em> for m samples, where <em>z1 = w.transpose * x1 + b</em> using just</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Z = np.dot(w.T, x) + b
</code></pre></div></div>

<p>And doing the same for <em>A</em> using some implementation of sigma.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/vectorising-logistic-regression.png" alt="screenshot" /></p>

<h1 id="vectorising-logistic-regressions-gradient-computation">Vectorising logistic regression’s gradient computation</h1>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/IgFnJ/vectorizing-logistic-regressions-gradient-output">Video #13</a> explains logistic regression for m samples using numpy.</p>

<p>It has <code class="language-plaintext highlighter-rouge">Z = np.dot(w.T, x) + b</code> from before, but adds</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = sigmoid(Z)
dZ = A - Y
dw = 1/m * X * dZ.T
db = 1/m * np.sum(dZ)
w := w - sigmoid(dw)
b := b - sigmoid(db)
</code></pre></div></div>

<p>Then he wraps it with a <code class="language-plaintext highlighter-rouge">range(1000)</code> outer loop, and I guess it will become more apparent later that these extra iterations gets your weights to a value where they “settle” (and you can probably break out of the loop if the delta becomes neglible).</p>

<h2 id="broadcasting-in-python">Broadcasting in Python</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/uBuTv/broadcasting-in-python">Video #14</a> basically means when using Python/numpy, when working with arrays and matrices, the code will intuitively do the right thing.</p>

<p>E.g. here you want to add 100 to the 4x1 column vector, but numpy will “broadcast” 100 into a 4x1 vector of 100s.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/broadcasting-example1.png" alt="screenshot" /></p>

<p>Ng refers to the <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">numpy docs on broadcasting</a>.</p>

<h2 id="a-note-on-pythonnumpy-vectors">A note on python/numpy vectors</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/87MUx/a-note-on-python-numpy-vectors">Video #15</a> talks about the pitfalls of working with rank-1 arrays, and urges you to be explicit about your matrix dimensions.</p>

<h2 id="quick-tour-of-jupyter">Quick tour of Jupyter</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/ChN1T/quick-tour-of-jupyter-ipython-notebooks">Video #16</a> introduces us to Jupyter (previously known as iPython notebooks).</p>

<h2 id="explanation-of-logistic-regression-cost-function-optional">Explanation of logistic regression cost function (optional)</h2>

<p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/SmIbQ/explanation-of-logistic-regression-cost-function-optional">Video #17</a> quickly explains why we like to use the cost function that we do for the logistic regression. Ng recaps the formulas for binary classification, and gets to using “maximum likelihood estimation” from statistics to derive our cost function.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/cost-function-explanation.png" alt="screenshot" /></p>

<p>And that’s a wrap for week 2.</p>

<h2 id="code-snippets">Code snippets</h2>

<p>Here follows some code snippets I picked up along the way.</p>

<p>https://gist.github.com/opyate/e28992cbaacf6623fa04b0455d72d25d</p>

<h2 id="what-else-ive-learned">What else I’ve learned</h2>

<h3 id="clean-and-pre-process">Clean and pre-process</h3>

<p>Before building a neural net, you’ll need to pre-process your data. (This is the same gist I picked up when doing my NN diploma back in 2001: cleaning and preparing data is sometimes the hardest part)</p>

<p>Common steps for pre-processing a new dataset are:</p>

<ul>
  <li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li>
  <li>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)</li>
  <li>“Standardize” the data, e.g. divide each pixel by 255</li>
</ul>

<p>The oval “neuron” in this diagram computes a linearity (Wx + b) and then an activation (sigmoid, tanh, ReLU) which is the prediction.</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/neural-net-one-example.png" alt="screenshot" /></p>

<h3 id="key-steps-to-building-a-model">Key steps to building a model</h3>

<ul>
  <li>define the model structure, such as number of input features</li>
  <li>initialize the parameters of the model, e.g. initialise the weights <em>w</em> to zeros or ones, or random numbers, and initialise the bias <em>b</em></li>
  <li>loop
    <ul>
      <li>forward propagation: calculate current loss; from <em>X</em> to <em>cost</em></li>
      <li>backward propagation: calculate current gradient</li>
      <li>update parameters (weights and bias): this is gradient descent!</li>
    </ul>
  </li>
</ul>

<p>You have a choice of activations</p>

<ul>
  <li>sigmoid</li>
  <li>tanh</li>
  <li>ReLU</li>
</ul>

<p>You have a choice of loss functions:</p>

<ul>
  <li>L1, or “least absolute deviations” (LAD), or “least absolute errors” (LAE)</li>
  <li>L2, or “least squares error” (LSE)</li>
  <li>L3? (as per week2) or “log-likelihood”?</li>
</ul>

<p>Good activations and loss functions seem to be “stumbled upon” empirically, and must be a topic of research.</p>

<p>Picking good hyperparameters (number of iterations, and learning rate) is an art?
Different learning rates give different costs and thus different predictions results.</p>

<p>An example of good/bad learning rates are illustrated below: good rate converges, and a bad rate diverges. (Images: Adam Harley)</p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/sgd.gif" alt="sgd" /></p>

<p><img src="/assets/posts/2017-11-22-week-2-of-neural-networks-and-deep-learning/sgd_bad.gif" alt="sgd_bad" /></p>

<p>If the learning rate is too large, the cost may oscillate up and down, or diverge completely. 
Lower cost isn’t necessarily better, and could lead to overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</p>

<p>More techniques in later weeks to reduce overfitting.</p>

<p>We have just built a single-node neural network, and from the NN course I did in 2001 I remember this being called a <a href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a>.</p>

</div>


  

  <div class="previousnext">

    
      <a class="pnbox prev" href="/blog/2017/11/21/week-1-of-neural-networks-and-deep-learning.html">&laquo; Week 1 of Neural Networks and Deep Learning</a>
    

    
      <a class="pnbox next" href="/blog/2017/11/23/the-purpose-of-life-isnt-to-be-happy.html">The purpose of life isn't to be happy &raquo;</a>
    

  </div>

  </div>
</div>


        </div>
      </main>

      <span>
    Copyright © 2002-2024 Juan Uys, <a href="https://github.com/juanuys/juanuys.github.io" alt="Github link to this website's source code.">(source code for this website)</a>.
    Updates via <a href="/feed.xml">RSS</a>, <a href="https://mastodon.art/@uys">Mastodon</a> or <a href="/newsletter">newsletter 💌</a>.
</span>

    </div>

  </body>

</html>
